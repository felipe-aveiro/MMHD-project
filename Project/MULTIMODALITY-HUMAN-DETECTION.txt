https://encord.com/blog/train-val-test-split/#:~:text=The%20optimal%20split%20ratio%20depends,10%2D20%25%20test%20data.

-- Might be interesting for data augmentation

https://labelstud.io/

-- Open source data labeling tool

Step 1)
    RGB - YOLO (3 channels)

Step 2)
    Thermal - YOLO (1 channel)
    â€¢ use 'trick' to expand to three channels

âœ… Fine-tuning Parameter:

    ğŸ“¦ epochs = 50

        ğŸ§  What it does:
            Specifies the number of times the model will go through the entire training dataset.
            Higher values allow the model to learn more â€” but can lead to overfitting if excessive.

        ğŸ“Œ Importance for fine-tuning:
            Very high â€” this controls how much the model adapts to your custom dataset.

        ğŸ“ˆ Typical tuning values:

            Small datasets: 50â€“200 epochs

            Large datasets: depends on compute resources and training time

        âœ… Yes, you can change this later. Just modify the value and re-run the training:

            python
                model.train(epochs=100, ...)
    
    ğŸ“¦ YOLO('yolov8s.pt')

        ğŸ§  What it does:
            This loads a pre-trained YOLOv8 model architecture with pretrained weights (.pt file).
            In this case: YOLOv8s â†’ "small" version of the model.

        ğŸ“Œ Importance for fine-tuning:
            Critical. Youâ€™re initializing the model with weights learned from the COCO dataset â€” meaning the model already has some understanding of object detection.

        ğŸ§¬ This is the starting point for fine-tuning â€” youâ€™ll then adapt it to your custom dataset (e.g., thermal or RGB humans only).

        ğŸ“Š Available Model Sizes:
        
            Model	        Size	Speed	         Accuracy	         Use Case

            yolov8n.pt	    nano	ğŸŸ¢ Fast	        ğŸ”´ Low	            Real-time or edge devices
            yolov8s.pt	    small	ğŸŸ¢ Fast	        ğŸŸ¡ Medium	        Balanced use cases
            yolov8m.pt	    medium	ğŸŸ¡ Medium	    ğŸŸ¢ Good	            More accurate, slower
            yolov8l.pt	    large	ğŸ”´ Slow	        ğŸŸ¢ High	            Heavy training, better accuracy
            yolov8x.pt	    x-large	ğŸ”´ Very slow	ğŸŸ¢ Best	            Research or high-end training

        âœ… You can switch models easily:

            python
                model = YOLO('yolov8n.pt')  # if you want speed
                model = YOLO('yolov8m.pt')  # if you want more accuracy

                        yolov8n.pt (nano)	        yolov8s.pt (small)

            Speed	    âš¡ Fastest	               ğŸ”„ Fast
            Size	    ğŸ’¾ Tiny (~3.2MB)	        ğŸ“¦ Small (~11MB)
            Accuracy	â— Lower	                    âœ… Better than nano
            Ideal for	Real-time, embedded	         Desktop, quick protos

            ** yolov8n is less accurate, so for proof of concept/benchmarks,
            you may still want to compare yolov8n vs yolov8s performance side-by-side.

    ğŸ“¦ patience=num_epochs

        ğŸ§  What it does:
            Controls early stopping â€” training will stop if validation loss doesnâ€™t improve after patience epochs.

        ğŸ“Œ Why it matters:
            Setting patience = num_epochs disables early stopping.
            This ensures the model trains for the full number of epochs, regardless of performance.

        â—  Recommendation:
            âœ… Good as-is while experimenting. You can lower later if overfitting starts.
    
    ğŸ“¦ pretrained=False

        ğŸ§  What it does:
            If False: fine-tunes from scratch using the weights in 'yolov8n.pt' (learned from COCO).
            If True: resumes training from a previous fine-tuned model (runs/.../weights/best.pt).

        ğŸ“Œ Why it matters:
            Setting False means youâ€™re doing transfer learning from generic COCO weights.
            If you already trained before and want to continue â€” set to True.

        â—  Recommendation:
            âœ… Leave it as False for your first full training.
    
    ğŸ“¦ imgsz=640

        ğŸ§  What it does:
            Sets the image resolution used during training (images will be resized to this).

        ğŸ“Œ Why it matters:
            Lower values â†’ faster training, but potentially less accurate.
            Higher values â†’ slower, but potentially better detection (especially for small objects).

        ğŸ”„ Tradeoff:
            416 for real-time focus
            640 (default) is balanced
            768+ if you need fine-grained detection

        â—  Recommendation:
            âœ… Stick with 640 for now. You can test others later during tuning.
